{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Back to top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='steelblue'> Predicting Elderly Out-of-pocket Expenditures for Health Care in the United States   </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name | SNR  |\n",
    "|------|------|\n",
    "| Emilie Bartels  | 2028466|\n",
    "| Larissa Heshusius  | 807104 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "<a href='#Preperations'>Preparations</a>\n",
    "\n",
    "<a href='#Research Question'>Introduction</a>\n",
    "\n",
    "<a href='#Motivation'>Motivation</a>\n",
    "\n",
    "<a href='#Data'>Data</a> \n",
    "\n",
    "<a href='#Methodology'>Methodology</a>\n",
    "\n",
    "<a href='#Preparing the data'>Preparing the data</a>\n",
    "\n",
    "<a href='#Results'>Results</a> \n",
    "\n",
    "- <a href='#Descriptive Statistics'>Descriptive Statistics</a> \n",
    "\n",
    "- <a href='#Linear Regression Model'>Multivariate Linear Regression</a> \n",
    "\n",
    "- <a href='#Lasso Regression'>Lasso Regression</a> \n",
    "    \n",
    "- <a href='#Random Forest Model'>Random Forest</a> \n",
    "\n",
    "<a href='#Discussion'>Discussion</a>\n",
    "\n",
    "<a href='#Conclusion'>Conclusion</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Preperations'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --user plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.tools as tls\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signing in to plotly\n",
    "py.sign_in('larissssx', 'Do9x3BGXFcDZImSTVwDa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the relevant libraries and modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import YouTubeVideo\n",
    "import sklearn as sk\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "from scipy.stats import pearsonr\n",
    "from statistics import stdev, mean\n",
    "from IPython.display import Image\n",
    "import scipy\n",
    "from scipy.cluster import hierarchy as hc\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import math\n",
    "\n",
    "# Hiding annoying warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Allowing plots to appear within the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting the number of decimals to zero\n",
    "pd.set_option('float_format', '{:.0f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Showing a list of available styles for the notebook\n",
    "print(plt.style.available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the 'seaborn-muted' style\n",
    "plt.style.use('seaborn-muted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Research Question'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This research is focused on the out of the pocket health medical expenditures of elderly in the United States (US), based on data from the HRS survey of 2014. We will use different supervised machine learning techniques like multivariate linear regresion, lasso regression and random forest regression to investigate the development of out-of pocket medical expenditures. To optimally analyze the data, we first prepare the data and look at some descriptive statistics to understand the data. We completely conduct our own research which differs from other researches that have been executed on the HRS dataset. We will evaluate the outcomes of our research to answer the following questions:\n",
    "\n",
    "**How can out-of pocket medical expenditures of elderly in the United States for individuals be predicted according to supervised machine learning techniques?** *\n",
    "\n",
    "Which personal characteristics influence out-of pocket medical expenditures most?\n",
    "\n",
    "Which model predicts the out of pocket medical expenditures of eldery best?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Motivation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***Development health care costs***\n",
    "\n",
    "Health care is an important topic in countries all over the world, since all people are affected by it and it accounts for a large part of the government expenditures. The costs significantly differ between people, but everyone gets in touch with it. Total health care expenditures are increasing every year, which is not always completely covered by the government. In a paper about supervised learning methods for predicting healthcare costs <a href='https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5977561/'>(Morid et al., 2018) </a> it is mentionted that the national health expenditure grew with 5.8% to 3,2 trillion dollar in 2015. This comes down to $9,900 per person which accounted for approximately 17.8 per cent of the nation’s GDP. \n",
    "\n",
    "Since the trend of rising health care costs will keep developing the upcoming years, it is important to control these unsustainable increases. To partially solve this, the future costs could be predicted to efficiently target care management for individuals at the highest risk of incurring these costs. One of these “sensitive” groups are the older, retired people. In the United States, like in many other countries, we are currently witnessing the maturation of the baby boom generation. Combined with an increasing life expectancy and declining birth rates, this results in fundamental changes in the demography of the country. Since this group is an important cause for the increasing health care costs, these costs can be one of the biggest expenses for this group and the costs keep growing. Therefore, this research is focused on people with an age of 70+ since a lot of information could be gained here.  \n",
    "\n",
    "\n",
    "***Individual medical expenditures***\n",
    "\n",
    "Cost prediction is not only important for health insurers, but also for the patients. The dependent variable in this research is out of pocket medical expenditures which are the costs for the individuals.  It is beneficial for them to know their likely expenditures in advance to optimally choose their insurance plan with the right deductibles and premiums. \n",
    "In this research, we investigate which demographic and personal characteristics affect the out of pocket health care costs for individuals. For example, previous research <a href='https://www.healthsystemtracker.org/chart-collection/health-expenditures-vary-across-population/#item-health-spending-increases-throughout-adulthood-men-women-spending-varies-age_2015'>(Kaiser Family Foundation, 2017) </a> has shown  that gender and race affect out of pocket expenditures. Some of these characteristics could be affected by the individuals, like smoking and alcohol consumption. When people know the effect of these factors, this could result in important policy implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "dataframe2 = pd.read_stata('Data/Dataset wave3-12.dta')\n",
    "\n",
    "# Creating a Pandas Dataframe\n",
    "df2 = pd.DataFrame(dataframe2)\n",
    "\n",
    "# Renaming the columns\n",
    "df2.rename(columns=\n",
    "          {'r10oopmd': '2010',\n",
    "           'r11oopmd' : '2012',\n",
    "           'r12oopmd' : '2014',\n",
    "           'r3oopmd' : '2006',\n",
    "           'r4oopmd' : '1998',\n",
    "           'r5oopmd' : '2000',\n",
    "           'r6oopmd' : '2002',\n",
    "           'r7oopmd' : '2004',\n",
    "           'r8oopmd' : '2006',\n",
    "           'r9oopmd' : '2008'\n",
    "          }, inplace=True) # the column is renamed in place\n",
    "\n",
    "# Changing the order of columns\n",
    "cols = list(df2.columns.values)\n",
    "df2 = df2[['1998', '2000', '2002', '2004', '2006', \\\n",
    "         '2008', '2010', '2012', '2014']]\n",
    "\n",
    "# Handling missing data\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(df2)\n",
    "df2 = pd.DataFrame(data=imp.transform(df2) , columns=df2.columns)\n",
    "\n",
    "# Visualizing the mean and standard deviation of out of pocket expenditures over the years\n",
    "p = means.plot(figsize=(8,8), legend=False,kind=\"bar\",rot=45,fontsize=14)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.ylabel('Average out of pocket expenditures (in US Dollars)', fontsize=16)\n",
    "plt.title('Average out of pocket medical expenditures over the years', fontsize=16)\n",
    "plt.ylim(0, 6000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = df2.std()\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> \n",
    "This bar chart shows that the average out of pocket medical expenditures of elderly in the United States peaked in 2004, decreased in 2006 and have increased ever since. The peak in 2004 can probably be explained by outliers, as the standard deviation is relatively high as well. In any case, the bar chart shows that the average out of pocket medical expenditures in the US are substantial. According to (<a href='https://www.nejm.org/doi/full/10.1056/NEJMsb1706645'>Sommers et al., 2017</a>), changes over the years can be explained by the following health care reforms:\n",
    "<br><br>\n",
    "- Emergency Medical Treatment and Active Labor Act (1986)<br>\n",
    "- Health Insurance Portability and Accountability Act (1996)<br>\n",
    "- Medicare Prescription Drug, Improvement, and Modernization Act (2003)<br>\n",
    "- Patient Safety and Quality Improvement Act (2005)<br>\n",
    "- Health Information Technology for Economic and Clinical Health Act (2009)<br>\n",
    "- Patient Protection and Affordable Care Act (2010)<br>\n",
    "<br>\n",
    "\n",
    "Under the latter, the federal government sets annual limits on the out-of-pocket spending maximums that apply to every healthcare plan sold in the United States. However, there are some requirements to be eligible for the Out-Of-Pocket Maximum Health Insurance Subsidy and some expenses do not count toward the out-of-pocket maximum limit (<a href='https://www.healthaffairs.org/doi/abs/10.1377/hlthaff.2015.0290'>Golberstein et al., 2015</a>).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Health & Retirement Study**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "This assignment explores the determinants of out-of pocket health care spending through the use of data from the <a href='http://hrsonline.isr.umich.edu/index.php?p=avail'>Health & Retirement Study U.S.</a> (HRS). The HRS is a longitudinal panel survey among Americans aged 50 and older. The study has been conducted every two years since 1992, and the most recent release is from the year 2016. The survey elicits information about a range of topics: demographics, income, assets, health, family structure, housing, job status and history, expectations, and insurance. Therewith, it is one of the most extensive academic social science projects ever undertaken. The study is administered and conducted by the Survey Research Center (SRC) at the University of Michigan. It is managed through a collaboration between the National Institute on Aging (NIA) and the Social Security Administration (SSA).\n",
    "\n",
    "The sample of the Health and Retirement Study is considered to be representative for the population they want to analyze, namely all Americans over 50. In total, the group of respondents consists of over 30,000 individuals in approximately 11,000 households. The complete HRS consists of seven cohorts in 2016, and its sample design over the years is as follows: \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"Data/Cohorts-Graph.png\", width=700, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> \n",
    "\n",
    "As set forth before, the HRS is extraordinarily comprehensive, but that also makes it more complex.Therefore, we will use the <a href='https://www.rand.org/well-being/social-and-behavioral-policy/centers/aging/dataprod/hrs-data.html'>RAND HRS Longitudinal File</a>, which is a user-friendly file derived from all waves. It contains cleaned and processed variables with consistent and intuitive naming conventions, model-based imputations, and spousal counterparts of most individual-level variables. \n",
    "\n",
    "For the purpose of this research, a narrower selection in the sample as well as variables will be made from this HRS datafile.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting relevant variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Selecting relevant variables'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> \n",
    "The original file consists of 11,465 variables for 37,495 respondents, but only a few of those are relevant for the research question at hand. Therefore, we provide an overview of the relevant variables, their definition, and measurement:<br>\n",
    "<br>\n",
    "*Target*<br>\n",
    "<br>\n",
    "\n",
    "- Out-of pocket medical expenditures: total expenses for medical care that were not reimbursed by insurance in the previous 2 years, reported in nominal US dollars. <br>\n",
    "<br>\n",
    "\n",
    "\n",
    "    Out-of-pocket costs include deductibles, coinsurance, and copayments for covered services plus all costs for services that aren't covered. The survey inclues nine categories of out-of-pocket medical costs: (1) hospital costs; (2) nursing home costs; (3) doctor visits costs; (4) dental costs; (5) outpatient surgery costs; (6) average monthly prescription drug costs; (7) home health care; (8) special facilities costs; and (9) other. <br>\n",
    "<br>    \n",
    "*Features*\n",
    "<br>\n",
    " \n",
    "- Age: calculated as the difference between the respondents’ birthdate and the beginning interview date. The age in years is the integer portion of the number of months old divided by 12.  \n",
    "<br>\n",
    "\n",
    "- Gender: the individual’s gender is labeled as either 1.male or 2.female.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "- Race: respondents were asked what they consider to be their primary race, either 1.white/Caucasian, 2.black/African American or 3.other.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "- Education: the number of academic years a person has completed in a formal program provided by elementary and secondary schools, universities, colleges or other formal post-secondary institutions.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "- Income: all income components are summed on the household level and reported in nominal US dollars. \n",
    "\n",
    "<br>\n",
    "- Children: provides the number of living children of the respondent and spouse or partner.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "- Self-reported health: a categorical variable reflecting the self-reported general health status. The respondents are asked to rate their health according to the categories 1 = excellent; 2 = very good; 3 = good; 4 = fair; and 5 = poor.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "- BMI: the respondent's body mass index reflects body weight adjusted for height. Height, given in feet and inches, is converted to meters. Weight is converted to kilograms. The final BMI is calculated by the following formula: $BMI = \\displaystyle\\frac{weight}{height^2}$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- Alcohol: indicates the number of days per week a respondent drinks alcoholic beverages.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- Smoking: indicates whether the respondent currently smokes cigarettes, labeled as either 0.no or 1.yes.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "- Covered by government: indicates whether the respondent is covered by any government health insurance program. It is labeled as either 0. no or 1. yes .\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "- Covered by employer: indicates whether the respondent is covered by health insurance from her/his current or previous employer. It is labeled as either 0.no or 1.yes.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "N.b. We have changed most of the labels for our own use when <a href='#Preparing the data'>preparing the data</a>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> There are cross-wave differences in the data. For example, components of out of pocket medical expenditures are added to the survey over the years. Furthermore, the reference period was prolonged from 1 year to 2 years from wave 3 onwards. In order to make a good prediction, we will focus on the 12th wave. This is the most recent wave for which we have all the relevant data available. The 12th wave contains data from the 2014 survey. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Selecting relevant respondents'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> \n",
    "\n",
    "**Selecting relevant respondents**\n",
    "<br>\n",
    "We will focus on the group of eldery between the age of 73 and 83 years old, belonging to the so-called 'orginal cohort' of the HRS (i.e. born in 1931-1941). This group is interesting because on the one hand they incur relatively high medical expenditures, but on the other hand often have a low income. It should be noted that this cohort in the population is unique in the sense that they often still have a financial responsibility for their children, whilst having to save for their own retirement days (<a href='https://blogs.kent.ac.uk/welfsoc/files/2015/05/Rakar-Tatjana-Future-responsibilities-towards-the-elderly-a-comparative-analysis-of-welfare-state-attitudes-and-expectations-in-Norway-and-Slovenia.pdf'>Hrast et al., 2016</a>). Additionally, older people are more vulnerable and more likely to need health care (<a href='https://academic.oup.com/biomedgerontology/article/70/11/1427/2605616'>Bandeen-Roche et al., 2015</a>).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Sample**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we extract the following variables from the <a href='https://www.rand.org/well-being/social-and-behavioral-policy/centers/aging/dataprod/hrs-data.html'>RAND HRS Longitudinal File</a>: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable | Code  | Type  |\n",
    "|------|------|------|\n",
    "| **Demographics, Identifiers, and Weights**  | **Section A**| |\n",
    "| Age  | r12agey_b | Continuous |\n",
    "| Gender  | ragender | Categorical |\n",
    "| Education  | raedyrs | Continuous |\n",
    "| Race  | raracem | Categorical |\n",
    "||||\n",
    "| **Health**  | **Section B** | |\n",
    "| Medical care utilization: Out of Pocket | r12oopmd | Continuous|\n",
    "| Self-reported health| r12shlt |Categorical|\n",
    "| BMI  | r12bmi | Continuous |\n",
    "| Alcohol  | r12drinkd | Categorical |\n",
    "| Smoking | r12smoken |Categorical|\n",
    "| **Income**  | **Section D** | |\n",
    "| Total household income  | h12itot | Continuous |\n",
    "||||\n",
    "| **Health Insurance**  | **Section G** | |\n",
    "|Covered by government|r12higov|Categorical|\n",
    "|Covered by former employer|r12prpcnt|Categorical|\n",
    "||||\n",
    "| **Family Structure**  | **Section H** | |\n",
    "|Number of children|h12child|Continuous|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section <a href='#Preparing the data'>preparing the data</a> we will elaborate on our approach to extract and filter this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Methodology'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> \n",
    "Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to effectively perform a specific task without using explicit instructions, relying on models and inference instead. ML is seen as a subset of artificial intelligence. Some applications of algorithms are face recognition, email spam and malware filtering, product recommendations, speech-to-text and natural language generation. Machine learning algorithms are often categorized as supervised or unsupervised (we will not discuss semi-supervised and reinforcement machine learning algorithms):<br><br>\n",
    "\n",
    "- Supervised algorithms require a data scientist or data analyst with machine learning skills to provide both input and desired output. Data scientists determine which variables, or features, the model should analyze and use to develop predictions. Once training is complete, the algorithm will apply what was learned to new data.<br>\n",
    "\n",
    "- Unsupervised algorithms do not need to be trained with desired outcome data. Instead, they use an iterative approach called deep learning to review data and arrive at conclusions. Unsupervised learning algorithms are used for more complex processing tasks than supervised learning systems.<br><br>\n",
    "\n",
    "We will use supervised machine learning, because our goal is to learn a function that, given a sample of data and desired outputs, best approximates the relationship between input (features) and output (out of pocket medical expenditures) observable in the data. <br><br>\n",
    "\n",
    "Supervised learning problems can be further grouped into regression and classification problems: <br>\n",
    "- Classification: it is a classification problem when the output variable is a category, such as “disease” and “no disease” or \"high risk\" and \"low risk\".<br>\n",
    "- Regression: it is a regression problem  when the output variable is a real value, such as “dollars” or “weight”.\n",
    "<br><br>\n",
    "As our output observable are out of pocket medical expenditures in US dollars, we face a regression problem.<br><br>\n",
    "\n",
    "Usually, when working on a machine learning problem with a given dataset, one tries different models and techniques to solve an optimization problem and fits the most accurate model, that will neither overfit nor underfit:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"Data/Fitting image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> \n",
    "For this assignment, we will use skicit-learn, a machine learning library for the Python programming language. It features various regression algorithms, including linear regression, lasso regression, and random forest  (<a href='http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf'>Pedregosa et al., 2011</a>).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multivariate Linear Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('CtKeHnfK5uA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> \n",
    "\n",
    "*Model*\n",
    "<br>\n",
    "A linear regression model fits a linear model with coefficients to minimize the residual sum of squares (RSS) between the observed responses in the dataset, and the responses predicted by the linear approximation. A multivariate linear regression model involves more than 1 predictor, thus has more than 1 slope coefficient.  <br><br>\n",
    "\n",
    "It takes the following form: <br><br>\n",
    "\n",
    "\\begin{gather*}\n",
    "Out\\; of\\; pocket\\; medical\\; expenditures =& β_0\\; +β_1\\; x\\; age+ β_2\\; x\\; \n",
    "gender+  β_3\\; x \\;race+  β_4 \\;x \\;education+  β_5\\; x \\;income\\  β_6 \\;x \\;children + β_7\\; x \\;health\n",
    "\\end{gather*}\n",
    "<br>\n",
    "\\begin{gather*}\n",
    "+  β_8 \\;x \\;bmi+  β_9\\; x \\;alcohol+  β_{10}\\; x \\;smoking+  β_{11}\\; x\\; covered \\;government+  β_{12} \\;x\\; covered\\; employer + \\epsilon\n",
    "\\end{gather*}\n",
    "<br>\n",
    "The  β  values are called the model coefficients. These values are 'learned' during the model fitting step using the least squares criterion. Then, the fitted model can be used to make predictions. <br><br>\n",
    "\n",
    "*Assumptions*<br>\n",
    "\n",
    "On the one hand, this model is fast, does not require tuning, is highly interpretable, and well-understood. On the other hand, it is unlikely to produce the best predictive accuracy. The model presumes a linear relationship between the features and response. If the relationship is highly non-linear, as with many scenarios, linear relationship will not effectively model the relationship and its prediction would not be accurate.<br><br>\n",
    "\n",
    "We will have to deal with the following issues:<br>\n",
    "\n",
    "- Non-constant variance of residuals (aka \"heteroskedasticity\"): indicated by funnel shape in residual plot --> transform the variable using a concave function: ln(y), sqrt(y). <br>\n",
    "\n",
    "\n",
    "- Outliers: plot studentized residuals: greater than 3 is an outlier --> try removing the observation from the dataset.<br>\n",
    "    \n",
    "\n",
    "- Multicollinearity: exists whenever there is a correlation between two or more predictors. Detect pairs of highly correlated variables by examining the correlation matrix for high absolute values --> try removing one of the correlated predictors from the model, or combining them into a single predictor.<br><br>\n",
    "\n",
    "It could be that we end up with a large number of features and relatively poor test score compared to the training score. This would be a problem of over-generalization or over-fitting. In that case, we will apply a lasso regression.<br><br>\n",
    "\n",
    "Source: <a href='https://books.google.nl/books?hl=en&lr=&id=GOVOCwAAQBAJ&oi=fnd&pg=PP1&dq=linear+regression+python&ots=NcbDPeYQ-I&sig=g8rdRBVIl3CEpiajVrRIvDFGsxE#v=onepage&q=linear%20regression%20python&f=false'>Raschka., 2009</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('qU1_cj4LfLY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_\"Everything should be made as simple as possible, but not simpler\"_ - Albert Einstein\n",
    "\n",
    "In other words, the best theory is the simplest one that still explains observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> \n",
    "*Model and its assupmtions*\n",
    "<br>\n",
    "The acronym “LASSO” stands for Least Absolute Shrinkage and Selection Operator. So it is no surprise that Lasso regression is a type of linear regression that uses shrinkage. It reduces model complexity and prevents over-fitting which may result from the previously discussed 'simple' linear regression. <br><br>\n",
    "\n",
    "Lasso regression does this by performing L1 regularization. The model adds a penalty equal to the absolute value of the magnitude of coefficients. This type of regularization can result in sparse models with few coefficients; some coefficients can become zero and eliminated from the model. <br><br>\n",
    "\n",
    "The goal of the algorithm is to minimize:\n",
    "<br><br>\n",
    "$SSE = \\sum_{i=1}^n (y_i - \\sum_{j}x_{ij} \\beta_j)^2 + \\lambda \\sum_{p}^{j=1} |\\beta_j| $<br><br>\n",
    "\n",
    "Some of the βs are shrunk to exactly zero, resulting in a regression model that’s easier to interpret. \n",
    "<br><br>\n",
    "The tuning parameter λ controls the strength of the L1 penalty. λ is basically the amount of shrinkage:<br>\n",
    "- When λ = 0, no parameters are eliminated. The estimate is equal to the one found with linear regression.<br>\n",
    "- As λ increases, more and more coefficients are set to zero and eliminated (theoretically, when λ = ∞, all coefficients are eliminated).<br>\n",
    "- As λ increases, bias increases.<br>\n",
    "- As λ decreases, variance increases.\n",
    "<br><br>\n",
    "If an intercept is included in the model, it is usually left unchanged.\n",
    "<br><br>\n",
    "The Lasso model does not respond well to outliers and should be used with caution in non-clean datasets. We will take care of this when <a href='#Preparing the data'>preparing the data</a>.\n",
    "<br><br>\n",
    "Source: <a href='http://papers.nips.cc/paper/3596-robust-regression-and-lasso.pdf'>Xy et al., 2009</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('D_2LkhMJcfY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> \n",
    "*Model*\n",
    "<br>\n",
    "The random forest model is one of the machine learning algorithms, a supervised learning algorithm. In general, the Random forest is a fast, simple and flexible tool which performs very well since training and predictions are fast due to the simplicity of underlying decision trees. The main idea of the model is that multiple decision tree will be build which will be merged together to get a more accurate and stable prediction. The algorithm of the Random Forest randomly selects observations and features from the data and builds various decision trees to average the results. Most of the time, the random forest is able to prevent of the time by creatin subsets of features and build smaller trees.\n",
    "With the model, we can easily measure the relative importance of each feature on the future prediction. <br><br>\n",
    "\n",
    "*Bagging method*<br>\n",
    "\n",
    "More randomness and diversity is integrated with the bagging method to feature space in the random forest method. The random forests are trained via this bagging method, which consists of randomly sampling subsets of the training data, which fits a model to he smaller data sets (fitting decsision trees to the subsets) and aggregrates the result. The idea of the bagging methods is that combining learning models increases the overall result and give some idea about the correctness of the model. \n",
    "<br><br>\n",
    "*Feature importance*<br>\n",
    "\n",
    "An important part of the model is its ability to measure the relative of the features on the prediction (the X variables in our  case). By looking at the importances of each variable, we can see which features do not contribute to the prediction process, since the sum of all importance equals 1. In this research we also use the from the Sklearn package, which measures the feature importance by looking at how much the tree nodes will reduce impority between all the trees in the random forest (<a href='https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd/'>Donges, 2018</a>).<br><br>\n",
    "\n",
    "\n",
    "*Assumptions*<br>\n",
    "- Since we deal with a continuous Y variable, we use the RandomForestRegressor instead of the Classifier. <br>\n",
    "- Predicted values at each node is the average response variable for the observations in the node <br>\n",
    "- Overfitting is a problem that may occur in machine learning (and random forest model) and could lead to worse performance of the model. If a model performs way better on a training set than the test set, it is likely that the model is overfitting and the trends in the data is too noisy. If the model is overfitting, we can be less accurate about the outcomes of the model. When more features are added to the model, this could also lead to overfitting.<br>\n",
    "- The accuracy of the model also depends on its hyperparameters. The random forest adds more randomness to the model while growing trees to search for the best feature among the random subset of features. in this research we use the standard parameters of the RandomForestRegressor. <br>\n",
    "- The random forest is a tool mainly used for predictions and not descriptions, so it is not the optimal approach to describe relationships in the data.<br>\n",
    "- The model can handle different types of feature types, so we can take this into account when handling the data.<br>\n",
    "- A disadvantage of the model is that the results are not easily interpretable and it can be hard to draw conclusions about the meaning of the model. Since we will produce a simple version of the model we take this into account.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Preparing the data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  Since the original dataset consists of 11,465 variables for 37,495 respondents, we cannot directly load this into Python. Therefore, we extracted the <a href='#Selecting relevant variables'>relevant variables</a> in Stata and subsequently load this dataset into the notebook. Any further preparations of the data in order to be able to analyze it, will be done in Python.</div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# STATA do-file\n",
    "\n",
    "* Setting the maximum number of variables to 67,000\n",
    "set maxvar 67000\n",
    "\n",
    "* Importing data\n",
    "use \"C:\\Users\\emili\\Google Drive\\Studie\\Tilburg University\\Courses\\Applied Economic Analysis I\\Assignment AEA\\Data\\randhrs1992_2014v2.dta\" \n",
    "\n",
    "* Selecting relevant variables\n",
    "keep r12agey_b ragender raedyrs r12oopmd r12bmi r12drinkd r12smoken r12shlt h12itot raracem h12child r12higov r12prpcnt\n",
    "\n",
    "* Saving the file\n",
    "save \"C:\\Users\\emili\\Google Drive\\Studie\\Tilburg University\\Courses\\Applied Economic Analysis I\\Assignment AEA\\Data\\\n",
    "> Dataset wave12 final set.dta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data\n",
    "dataframe = pd.read_stata('Data/Dataset wave12 final set.dta')\n",
    "\n",
    "# Creating a Pandas Dataframe\n",
    "df = pd.DataFrame(dataframe)\n",
    "\n",
    "# Getting a first look\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  It is often said that 80% of the effort of analysis is in data cleaning. The paper <a href='https://www.researchgate.net/publication/215990669_Tidy_data'>Tidy data </a> by Hadley Wickham (2014) offers a set of tools that are useful to deal with a large number of messy data sets.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns\n",
    "df.rename(columns=\n",
    "          {'ragender': 'gender',\n",
    "           'raedyrs' : 'education',\n",
    "           'r12agey_b' : 'age',\n",
    "           'r12shlt' : 'health',\n",
    "           'r12bmi' : 'bmi',\n",
    "           'r12smoken' : 'smoking',\n",
    "           'r12drinkd' : 'alcohol',\n",
    "           'r12oopmd' : 'out of pocket',\n",
    "           'h12itot' : 'income',\n",
    "           'raracem' : 'race',\n",
    "           'h12child' : 'children',\n",
    "           'r12higov' : 'covered_government',\n",
    "           'r12prpcnt' : 'covered_employer',           \n",
    "          }, inplace=True) # the column is renamed in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspecting the data\n",
    "print('This dataframe consists of',df.shape[0],'rows and',df.shape[1],'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Counting missing values\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> We have missing values in each column except for gender. We will handle this <a href='#later on'>later on</a> in the notebook, after we have made all of our variables ready for analysis.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting categorical and ordinal features into numeric features\n",
    "replacements0 = {\n",
    "    '1.white/caucasian': 1,\n",
    "    '2.black/african american': 2,\n",
    "    '3.other' : 3\n",
    "}\n",
    "df['race'].replace(replacements0, inplace=True)\n",
    "\n",
    "replacements1 = {\n",
    "  '0.none':0,\n",
    "  '17.17+ yrs': 17\n",
    "}\n",
    "df['education'].replace(replacements1, inplace=True)\n",
    "\n",
    "replacements2 = { \n",
    "  '5.poor': 1, \n",
    "  '4.fair': 2,\n",
    "  '3.good': 3,\n",
    "  '2.very good': 4,\n",
    "  '1.excellent': 5\n",
    "}\n",
    "df['health'].replace(replacements2, inplace=True)\n",
    "\n",
    "replacements3 = {\n",
    "  '0.0 or doesnt drink': 0\n",
    "}\n",
    "df['alcohol'].replace(replacements3, inplace=True)\n",
    "\n",
    "replacements4 = {\n",
    "  '0.no':0,\n",
    "  '1.yes': 1\n",
    "}\n",
    "df['covered_government'].replace(replacements4, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  The fourth replacement in the cell above effectively creates a dummy variable, which can only take a value of 0 or 1. Another way to create a dummy variables uses the map-function:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables \n",
    "df['gender'] = df.gender.map({'2.female':0, '1.male':1})\n",
    "df['smoking'] = df.smoking.map({'0.no': 0, '1.yes': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df[['out of pocket', 'age', 'gender', 'race', 'education', 'income','children',\\\n",
    "      'health', 'bmi', 'alcohol', 'smoking', 'covered_government', 'covered_employer']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='before'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  We have to create mulptiple dummy variables for the  variables 'race', 'health', and 'alcohol', because these have multiple categories.\n",
    "<br><br>\n",
    "We have to exclude the first dummy column for the linear regression. Leaving the first column out as a reference, we are effectively setting the baselines to 'Caucasians', 'poor health', and '0' alcohol. We can do this, because if we know values of k-1 dummies in the data we automatically know the values of that last one dummy. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating multiple dummy variables using get_dummies, then excluding the first dummy column\n",
    "race_dummies = pd.get_dummies(df.race, prefix='race').iloc[:, 1:]\n",
    "health_dummies = pd.get_dummies(df.health, prefix='health').iloc[:, 1:]\n",
    "alcohol_dummies = pd.get_dummies(df.alcohol, prefix='alcohol').iloc[:, 1:]\n",
    "\n",
    "\n",
    "# Concatenating the dummy variable columns onto the DataFrame df3\n",
    "df = pd.concat([df, race_dummies], axis=1)\n",
    "df = pd.concat([df, health_dummies], axis=1)\n",
    "df = pd.concat([df, alcohol_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Changing the order of columns\n",
    "cols = list(df.columns.values)\n",
    "df = df[['out of pocket', 'age', 'gender', 'race_2.0', 'race_3.0', 'education', 'income','children',\\\n",
    "      'health_2.0', 'health_3.0', 'health_4.0', 'health_5.0', 'bmi', 'alcohol_1.0', 'alcohol_2.0', 'alcohol_3.0', \\\n",
    "      'alcohol_4.0',  'alcohol_5.0', 'alcohol_6.0', 'alcohol_7.0', 'smoking', 'covered_government', 'covered_employer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Retrieving data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">   Notice the value _inf_ (infinity), which is a value that is greater than any other value. In contrast, the value _-inf_ is  smaller than any other value.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Handling infinite values\n",
    "df.replace([np.inf], np.nan, inplace=True)\n",
    "df.replace([-np.inf], np.nan, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='later on'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NaN stands for \"Not a Number\", so it is not possible to do arithmetic with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  As seen before, we have missing values in each column except for gender, but our models cannot handle missing data. The simplest resolution would be to remove observations that have missing data. However, removing missing data can introduce a lot of issues. When data is randomly missing, you potentially lose a lot of your data. When data is non-randomly missing, in addition to losing data, you are also introducing potential biases. When applying this approach, we would lose almost 50 per cent of our data (we would be left with 18,155/37,495 entries). Therefore, this solution would not be optimal.\n",
    "<br><br>\n",
    "An alternative solution is to use imputation by replacing missing values with another value. There are many options we could consider when replacing a missing value, for example a random value, mean, median, mode or a value estimated by another predictive model. We opt for the median, because the dataset has great outliers.\n",
    "<br><br>\n",
    "We use the scikit-learn library, which provides the Imputer() pre-processing class that can be used to replace missing values. The Imputer class operates directly on the NumPy array instead of the DataFrame.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing data\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "imp.fit(df)\n",
    "df = pd.DataFrame(data=imp.transform(df) , columns=df.columns)\n",
    "\n",
    "# Recounting missing values \n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  For the purpose of this research, and explained in the section about <a href='#Selecting relevant respondents'>selecting relevant respondents</a>, we only want to look at people from the 3rd cohort aged between 73 and 83 years old.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~(df['age'] <= 72)] \n",
    "df = df[~(df['age'] >= 84)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will visualize some variables to detect the presence of outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df[\"age\"], y=df[\"out of pocket\"], data=pd.melt(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df[\"bmi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df[\"income\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> An outlier is an observation that deviates drastically from other observations in a dataset. From these plots it is obvious that the variables 'income', 'bmi' and 'out of pocket' have outliers. \n",
    "<br><br>\n",
    "We will use the Interquartile Range (IQR) to detect and remove the largest outliers. Any point which falls more than 1.5 times the interquartile range above the third quartile or below the first quartile will be seen as an outlier and thus be removed.</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a set of variables that the IQR has to apply to\n",
    "df3 = df[['income', 'bmi', 'out of pocket']]\n",
    "\n",
    "# Calculating the first and third quartile\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "\n",
    "# Calculating interquartile range\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Calculating the outlier cutoff\n",
    "cut_off = IQR * 1.5\n",
    "lower = Q1 - cut_off\n",
    "upper = Q3 + cut_off\n",
    "\n",
    "# Removing outliers\n",
    "df = df[~((df3 < lower) |(df3 > upper)).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it should be clear from the same plots, but with filtered data, that the outliers have indeed be removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df[\"age\"], y=df[\"out of pocket\"], data=pd.melt(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df[\"education\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df[\"bmi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df[\"income\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  A train/test split is a method for splitting our dataset into two groups: a training group of data-points that will be used to train the model, and a testing group that will be used to test it. It is usually split inequaly, because training the model requires as much data-points as possible. A ratio of 80/20 for train/test is common.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing of X data set containing the prediction variables as well as the vector y containing the data to be predicted.\n",
    "X = df.drop('out of pocket', axis=1)\n",
    "Y = df['out of pocket']\n",
    "\n",
    "# Splitting of X and Y into two parts each, which will be used for training (80%) and testing (20%) the model.\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.2, random_state = 1) \n",
    "\n",
    "print ('Number of training data:',len(X_Train))\n",
    "print ('Number of testing data:',len(X_Test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the shape of X and Y\n",
    "print(X_Train.shape)\n",
    "print(X_Test.shape)\n",
    "print(Y_Train.shape)\n",
    "print(Y_Test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  We see that the 80/20 train/test ratio succeeded. We have 3000 observations in the train set, and 750 observations in the test set. There are 22 features to predict the out of pocket medical expenditures.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Results'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Descriptive Statistics'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> Some machine learning algorithms like linear and logistic regression can suffer poor performance if there are highly correlated attributes in your dataset. As such, it is a good idea to review all of the pair-wise correlations of the attributes in your dataset. You can use the corr() function on the Pandas DataFrame to calculate a correlation matrix. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The final training dataframe consists of',X_Train.shape[0],'rows and',X_Train.shape[1],'columns')\n",
    "print('The final test dataframe consists of',X_Test.shape[0],'rows and',X_Test.shape[1],'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr()[['out of pocket']].iloc[1:])\n",
    "print('correlation with respect to out of pocket expenditures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the correlations of all features variables in our data with respect to out of pocket, which is interesting to see which variables move in the same direction. The legenda is adjusted to the data since the correlations are not higher than approximately 0.12. We can see that income, education, coveredEmployer, race2.0 and smoking show the most outstanding correlations. Income, education and coveredEmployer are positively correlated to out of pocket expenditures and race2.0 and smoking are negatively correlated.\n",
    "\n",
    "The next figure shows the correlations between the features, where we picked the most relevant ones to give a clear view of the correlations. For example, it is not relevant to see the correlation between Health 3.0 and Health 4.0.\n",
    "We can see that income and education are (relatively) highly correlated to each other,  which makes sense since people with a higher education have higher earnings in general and they are both positively correlated to out of pocket. However, correlations do not necessarily imply causation so we cannot draw conclusions from this and many factors can play a role which are not taken into account.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Pearson's correlations\n",
    "plt.figure(figsize=(12,12)) \n",
    "sns.heatmap(df[['out of pocket', 'age', 'gender', 'race_2.0', 'race_3.0', 'education', 'income','children', 'bmi', \\\n",
    "'smoking', 'covered_government', 'covered_employer']].corr(method='pearson'), linewidth=0.2,vmax=1.0,square=True, linecolor='steelblue', annot=True)\n",
    "print('Correlations between the variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(5, 2, figsize=(25, 18))   \n",
    "\n",
    "df['smoking'].value_counts().plot.bar(ax=axarr[0][0], fontsize=16, color='mediumvioletred')\n",
    "axarr[0][0].set_title(\"Smoking\", fontsize=16)\n",
    "df.groupby('smoking')['out of pocket'].mean().plot.bar(y = \"out of pocket\" ,color='mediumvioletred',ax=axarr[0][1],fontsize=16)\n",
    "axarr[0][1].set_title(\"Average oop costs\", fontsize=16)\n",
    "\n",
    "df['gender'].value_counts().plot(ax=axarr[1][0], fontsize=16, color='mediumvioletred', kind='bar')\n",
    "axarr[1][0].set_title(\"Gender\", fontsize=16)\n",
    "df.groupby('gender')['out of pocket'].mean().plot.bar(ax=axarr[1][1],color='mediumvioletred',fontsize=16)\n",
    "axarr[1][1].set_title(\"Average oop cost\", fontsize=16)\n",
    "\n",
    "df['age'].value_counts().plot.bar(ax=axarr[2][0], fontsize=16, color='mediumvioletred')\n",
    "axarr[2][0].set_title(\"Age\", fontsize=16)\n",
    "df.groupby('age')['out of pocket'].mean().plot.bar(y = \"out of pocket\" ,color='mediumvioletred',ax=axarr[2][1], fontsize=16)\n",
    "axarr[2][1].set_title(\"Average oop cost\", fontsize=16)\n",
    "\n",
    "df['education'].value_counts().plot.bar(ax=axarr[3][0], fontsize=16,color='mediumvioletred')\n",
    "axarr[3][0].set_title(\"Education\", fontsize=16)\n",
    "df.groupby('education')['out of pocket'].mean().plot.bar(ax=axarr[3][1],color='mediumvioletred',fontsize=16)\n",
    "axarr[3][1].set_title(\"Average oop cost\", fontsize=16)\n",
    "\n",
    "df['children'].value_counts().plot.bar(ax=axarr[4][0], fontsize=16,color='mediumvioletred')\n",
    "axarr[4][0].set_title(\"Children\", fontsize=16)\n",
    "df.groupby('children')['out of pocket'].mean().plot.bar(ax=axarr[4][1],color='mediumvioletred',fontsize=16)\n",
    "axarr[4][1].set_title(\"Average oop cost\", fontsize=16)\n",
    "plt.subplots_adjust(hspace=.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  From the plots above we can see that some variables are quite distributed. If we look at smoking we can see that only a small fraction of the elderly smoke. This is surprising, since we expected to have a high group of smokers because in the past it was normal for people to smoke, so also for this generation. The average out of pocket costs are also higher for the non-smokers, but we can probably not draw conclusions with this dataset of smoking on out of pocket costs since the group smokers is too small. \n",
    "Another striking conclusion is the distribution of education, since the plot shows that most of the people went to school for 12 years. A potential reason might be due that people did continue studying after high school. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [go.Histogram(x=df['bmi'])] \n",
    "py.iplot(data, filename = 'basic-line', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = [go.Histogram(x=df['income'])] \n",
    "py.iplot(data, filename = 'income', auto_open=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> OLS or Ordinary Least Squares OLS is a statistical method of finding the relationship between independent and dependent variables. The objective is to minimize the error between the data points (observed) and the points on the line (predicted):\n",
    "<br><br>\n",
    "\n",
    "$SSE = \\sum_{i=1}^{n} (y_i - \\hat{y})^2$\n",
    "\n",
    "<br><br>\n",
    "It is a relatively simple application of linear algebra and optimization techniques that show up all the time in machine learning. We will run an OLS to get a first look of the size and significance of the several features in their effect on out of pocket medical expenditures.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running an OLS\n",
    "X_Train1 = X_Train\n",
    "X_Train1 = pd.concat([pd.DataFrame(np.ones((X_Train1.shape[0], 1)).astype(int),\n",
    "                                  index=X_Train1.index), X_Train1], axis=1)\n",
    "X_Train1.rename(columns={0: 'const'}, inplace=True)\n",
    "\n",
    "X_opt = X_Train1[['age', 'gender', 'race_2.0', 'race_3.0', 'education', 'income','children',\\\n",
    "      'health_2.0', 'health_3.0', 'health_4.0', 'health_5.0', 'bmi', 'alcohol_1.0', 'alcohol_2.0', 'alcohol_3.0', \\\n",
    "      'alcohol_4.0',  'alcohol_5.0', 'alcohol_6.0', 'alcohol_7.0', 'smoking', 'covered_government', 'covered_employer']]\n",
    "OLS = sm.OLS(endog = Y_Train, exog = X_opt).fit()\n",
    "OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The R-square score for the OLS: {:.2f}'.format(OLS.rsquared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  The R-squared is 50%, so these features explain 50% of the variation within out of pocket medical expenditures. 11 coefficients are significant at a 5% significance level. This OLS confirms the literature that healt costs increase with age, that African American people incure less health costs, and that an increasing BMI is associated with more out of pocket medical expenditures. Especially the coefficients of self-reported health are interesting. It shows that each category of better self-reported health decreases the out of pocket medical expenditures with more US dollars with respect to a poor health.\n",
    "<br><br>\n",
    "However, the effect of gender (literature finds women are healthier than men), education (more education is normally associated with a healthier lifestyle), and smoking (should decrease your health and thus increase health costs) have opposite effects of what we would expect. Finally, alcohol has an ambiguous effect on out of pocket medical expenditures.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Linear Regression Model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">   As mentioned <a href='#before'>before</a>, we do have to exclude the first dummy column for the linear regression. Leaving the first column out as a reference, we are effectively setting the baselines to 'Caucasians', 'poor health', and '0' alcohol. We can do this, because if we know values of k-1 dummies in the data we automatically know the values of that last one dummy. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting and predicting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fitting the model to the training data \n",
    "lr = LinearRegression()\n",
    "lr.fit(X_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the testing set\n",
    "y_pred = lr.predict(X_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a prediction  of out of pocket medical expenditures for a person with the following characteristics:\n",
    "\n",
    "- age = 82 years\n",
    "- gender = male\n",
    "- race = black/African American\n",
    "- education = 10 years\n",
    "- income = 25000$\n",
    "- children = 2\n",
    "- health = very good\n",
    "- bmi = 24\n",
    "- alcohol = 3 days a week)\n",
    "- smoking = yes\n",
    "- covered by the government = yes\n",
    "- covered by the employer = no "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr.predict([82,1,1,0,10,25000,4,1,0,0,0,24,0,0,1,0,0,0,0,1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could compare this to a prediction of someone with similar characteristics, but e.g. without being covered by the government and with a higher bmi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict([82,1,1,0,10,25000,4,1,0,0,0,32,0,0,1,0,0,0,0,1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As could be expected, this results in higher predicted out of pocket medical expenditures (over a 2-year period)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now obtain the exact values for the estimated coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating coefficients\n",
    "coeff = DataFrame(X_Train.columns)\n",
    "coeff['Coefficient Estimate'] = Series(lr.coef_)\n",
    "coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  \n",
    "Most of the coefficients seem to have the same direction as in the OLS. Let's interpret:\n",
    "<br><br>\n",
    "Ceteris paribus, each year increase in age is associated with a 19$ increase of out of pocket medical expenditures for two years. Men spend 242 US dollars less in two years on out of pocket medical expenditures, compared to women. \n",
    "<br><br>\n",
    "Both being black/African American or another is associated with a decrease, 288 and 187 US dollars respectively, in out of pocket medical expenditures as compared to being white/Caucasian, which is the baseline level.\n",
    "<br><br>\n",
    "It is notable that income does not seem to have an effect on out of pocket medical expenditures. \n",
    "<br><br>\n",
    "Interestingly, each year of extra education and currently being a smoker are associated with a decrease in out of pocket medical expenditures. \n",
    "<br><br>\n",
    "Just like in the OLS, a better degree of self-reported health is associated with less out of pocket medical expenditures.\n",
    "<br><br>\n",
    "Alcohol has an ambiguous effect, as it is different for all categories. \n",
    "<br><br>\n",
    "Finally, being covered by the governments gives lower out of pocket medical expenditures compared to not being covered, whereas coverage by a former employer is associated with higher out of pocket medical expenditures.\n",
    "<br><br>\n",
    "It is important to note that these are a statements of association, not causation. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  We will use the R-square and mean squared error to evaluate the model. <br><br>\n",
    "\n",
    "The R-square determines how much of the total variation in Y (target variable) is explained by the variation in X (features). The value of R-square is always between 0 and 1, where 0 means that the model does not model explain any variability in the target variable (Y) and 1 meaning it explains full variability in the target variable. Now let us check the R-square for the above model.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = sk.metrics.r2_score(Y_Test, y_pred)\n",
    "print('The R-square score for the multivariate linear regression model is: {:.2f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, R² is 2%, meaning, only 2% of variance in out of pocket medical expenditures is explained by the features included in the model. In other words, if you know someone's features, you will have 2% information to make an accurate prediction about his or her out of pocket medical expenditures for a 2-year period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculating the mean square error \n",
    "lrmse = mean_squared_error(Y_Test,y_pred) \n",
    "\n",
    "# Calculating the R square\n",
    "lrr2_score = r2_score(Y_Test,y_pred) \n",
    "\n",
    "print('The mean square error for the multivariate linear regression model is: {:.2f}'.format(lrmse))\n",
    "print('The R-square score for the multivariate linear regression model is: {:.2f}'.format(lrr2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score on the Train set\n",
    "print(\"Score on the Train set:\", lr.score(X_Train, Y_Train))\n",
    "\n",
    "# Score on the Test set\n",
    "print(\"Score on the Test set:\",lr.score(X_Test, Y_Test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some overfitting going on, as the R-square is three times higher for the Train set. But in general, the R-square just is not very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> Residual plots also help evaluate and improve the regression model. A residual is the difference between the observed value of the dependent variable (y) and the predicted value (ŷ). The following plot tests the assumptions of whether the relationship between your variables is linear (i.e. linearity) and the whether there is equal variance along the regression line (i.e. homoscedasticity).A “good” residuals vs. fitted plot should be relatively shapeless without clear patterns in the data, no obvious outliers, and be generally symmetrically distributed around the 0 line without particularly large residuals.\n",
    "<br><br>\n",
    "So let's generate a residual vs fitted values plot:\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_plot = plt.scatter(y_pred, (y_pred - Y_Test), c='b')\n",
    "plt.hlines(y=0, xmin= -1000, xmax=5000)\n",
    "plt.title('Residual plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  We can see a funnel like shape in the plot. This shape indicates Heteroskedasticity. The presence of non-constant variance in the error terms results in heteroskedasticity. We can clearly see that the variance of error terms(residuals) is not constant. Generally, non-constant variance arises in presence of outliers or extreme leverage values. These values get too much weight, thereby disproportionately influencing the model’s performance. When this phenomenon occurs, the confidence interval for out of sample prediction tends to be unrealistically wide or narrow. \n",
    "\n",
    "The scatter plot indicates signs of non linearity in the data which has not been captured by the model. In order to capture this non-linear effects, one could try a polynomial regression. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Lasso Regression'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we check the magnitude of coefficients?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the magnitude of coefficients\n",
    "predictors = X_Train.columns\n",
    "coef = Series(lr.coef_,predictors).sort_values()\n",
    "coef.plot(kind='bar', title='Modal Coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  We can see that coefficients of excellent and very good health are much higher as compared to rest of the coefficients. Hence, someone's out of pocket medical expenditues would be more driven by these two features.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lasso001 = Lasso(alpha=0.01, normalize = True)\n",
    "lasso001.fit(X_Train,Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_score001=lasso001.score(X_Train,Y_Train)\n",
    "test_score001=lasso001.score(X_Test,Y_Test)\n",
    "print(\"Training score: {:.3f}\".format(train_score001))\n",
    "print(\"Test score: {:.3f}\".format(test_score001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_used001 = np.sum(lasso001.coef_!=0)\n",
    "print(\"Number of features used {:.0f}\".format(coeff_used001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lasso03 = Lasso(alpha=0.3, normalize = True)\n",
    "lasso03.fit(X_Train,Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score03=lasso03.score(X_Train,Y_Train)\n",
    "test_score03=lasso03.score(X_Test,Y_Test)\n",
    "print(\"Training score: {:.3f}\".format(train_score03))\n",
    "print(\"Test score: {:.3f}\".format(test_score03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coeff_used03 = np.sum(lasso03.coef_!=0)\n",
    "print(\"Number of features used {:.0f}\".format(coeff_used03))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has dropped 2 features, and the R-square for the test set increased accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "lasso.fit(X_Train,Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score1=lasso.score(X_Train,Y_Train)\n",
    "test_score1=lasso.score(X_Test,Y_Test)\n",
    "print(\"Training score: {:.3f}\".format(train_score1))\n",
    "print(\"Test score: {:.3f}\".format(test_score1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_used1 = np.sum(lasso.coef_!=0)\n",
    "print(\"Number of features used {:.0f}\".format(coeff_used1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no improvement in the scores, nor were any features dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  On the whole, the training and test scores are similar to the basic linear regression case. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following graph clearly shows that excellent health is the most important variable in terms of predicting out of pocket medical expenditures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the coefficients\n",
    "lasso_coef = lasso.coef_\n",
    "print(lasso_coef)\n",
    "\n",
    "# Plot the coefficients\n",
    "colnames = X.columns\n",
    "plt.plot(range(len(colnames)), lasso_coef)\n",
    "plt.xticks(range(len(colnames)), colnames.values, rotation=60) \n",
    "plt.margins(0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> The most common approach to find the best-trained algorithm is K-fold cross validation. In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The advantage of this method over repeated random sub-sampling (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. We will use the commonly chosen K=5: </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear regression object: reg\n",
    "reg = LinearRegression()\n",
    "\n",
    "# Compute 5-fold cross-validation scores: cv_scores\n",
    "cv_scores = cross_val_score(reg, X_Train, Y_Train, cv=5)\n",
    "\n",
    "# Print the 5-fold cross-validation scores\n",
    "print(cv_scores)\n",
    "\n",
    "# find the mean of our cv scores here\n",
    "print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following graph will visualize this by showing how picking a different alpha score changes the R-square:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of alphas and lists to store scores\n",
    "alpha_space = np.logspace(-4, 0, 50)\n",
    "lasso9_scores = []\n",
    "lasso9_scores_std = []\n",
    "\n",
    "# Create a lasso regressor: lasso9\n",
    "lasso9 = Lasso(normalize=True)\n",
    "\n",
    "# Compute scores over range of alphas\n",
    "for alpha in alpha_space:\n",
    "\n",
    "    # Specify the alpha value to use: lasso.alpha\n",
    "    lasso9.alpha = alpha\n",
    "    \n",
    "    # Perform 10-fold CV: lasso09_cv_scores\n",
    "    lasso9_cv_scores = cross_val_score(lasso9, X_Train, Y_Train, cv=10)\n",
    "    \n",
    "    # Append the mean of lasso09_cv_scores to ridge_scores\n",
    "    lasso9_scores.append(np.mean(lasso9_cv_scores))\n",
    "    \n",
    "    # Append the std of lasso09_cv_scores to ridge_scores_std\n",
    "    lasso9_scores_std.append(np.std(lasso9_cv_scores))\n",
    "\n",
    "# Use this function to create a plot    \n",
    "def display_plot(cv_scores, cv_scores_std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(alpha_space, cv_scores)\n",
    "\n",
    "    std_error = cv_scores_std / np.sqrt(10)\n",
    "\n",
    "    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n",
    "    ax.set_ylabel('CV Score +/- Std Error')\n",
    "    ax.set_xlabel('Alpha')\n",
    "    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n",
    "    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n",
    "    ax.set_xscale('log')\n",
    "    plt.show()\n",
    "\n",
    "    # Display the plot\n",
    "display_plot(lasso9_scores,lasso9_scores_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest score we can achieve is a R-squared of 0.036, which is still really low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Random Forest Model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100)\n",
    "rf.fit(X_Train, Y_Train)\n",
    "rf.score(X_Train, Y_Train)\n",
    "acc_rf = round(rf.score(X_Train, Y_Train) * 100, 2)\n",
    "print(round(acc_rf,2,), \"%\")\n",
    "rf.score(X_Train, Y_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  We already defined the Train and Test set, <a href='#Construct Test and Train data'>Construct Test and Train data</a> which we will use for the Random Forest Model. rf.fit is used to pass in the independent variables and our dependent variable out of pocket. The rf.score shows the $R^2$ of the trainings variables, which looks good since it is quiet high (1 is the highest), but this is for the training set.<br><br>\n",
    "n_estimators shows the number of trees the forest has. When the n_estimators is higher, the model might be more accurate. In general, n_estimators will be set between 60 and 120 in most models since on a certain point the model does not improve anymore and only becomes slower.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(rf):\n",
    "    res = [rf.score(X_Train, Y_Train), rf.score(X_Test, Y_Test)]\n",
    "    if hasattr(rf, 'oob_score_'): res.append(rf.oob_score_)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rf.fit(X_Train, Y_Train)\n",
    "print_score(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  %time shows the time it takes to run for the computer since it should not run for too long. In this case this is not a problem/.\n",
    "However, this $R^2$ did not give a complete view of how good our model is. Unfortunately the $R^2$ for the test set is way lower (4th column) and is behaving poorly. The $R^2$ is even negative, which means the outcome of the test set is really bad and the model is worse than predicting the mean. A reason for this problem is that the model might be overfitting and the training model is not representative. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Other version_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor(n_estimators = 300, random_state = 1,n_jobs = -1, oob_score=True)\n",
    "forest.fit(X_Train,Y_Train)\n",
    "forest_train_pred = forest.predict(X_Train)\n",
    "forest_test_pred = forest.predict(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE train data: %.3f, MSE test data: %.3f' % (\n",
    "mean_squared_error(Y_Train,forest_train_pred),\n",
    "mean_squared_error(Y_Test,forest_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R2 train data: %.3f, R2 test data: %.3f' % (\n",
    "r2_score(Y_Train,forest_train_pred),\n",
    "r2_score(Y_Test,forest_test_pred)))\n",
    "print_score(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  Since the outcomes are really bad, we also tried another forest but it keeps the same problems. n_jobs tells how many processors could be used, and when it is equal to -1 there is no limit. The oob_score is also set to \"true\", this is another random forest cross validation method (out of bag sampling). The oob score is also negative, which again confirmst the problems with our model.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = {\n",
    "    'data': [\n",
    "        {'x': Y_Train, 'y': forest_train_pred - Y_Train, 'mode': 'markers', 'name': 'Train data'},\n",
    "        {'x': Y_Test, 'y': forest_test_pred - Y_Test, 'mode': 'markers', 'name': 'Test data'}\n",
    "    ],\n",
    "    'layout': {\n",
    "        'xaxis': {'title': 'Reported Y'},\n",
    "        'yaxis': {'title': \"Difference\"}\n",
    "    }\n",
    "}\n",
    "py.iplot(fig, filename='hoiii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the difference between the reported Y and the predicted Y for all test and trainings data.\n",
    "We can see that for low reported Y's the difference is small and slightly positive in the training set. For higher values of Y, approximately Y>2500, the predicted Y is lower than the actual reported values of Y.\n",
    "This plot also shows that the training data works quite well (untill a reported Y of 4500), but the results of the test\n",
    "data is way worse. They move in the same direction, but the differences are higher for the test set which shows that \n",
    "the predictions are less accurate for this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = {\n",
    "    'data': [\n",
    "        {'x': forest_train_pred, 'y': forest_train_pred - Y_Train, 'mode': 'markers', 'name': 'Train data'},\n",
    "        {'x': forest_test_pred, 'y': forest_test_pred - Y_Test, 'mode': 'markers', 'name': 'Test data'}\n",
    "    ],\n",
    "    'layout': {\n",
    "        'xaxis': {'title': 'Predicted Y'},\n",
    "        'yaxis': {'title': \"Difference\"}\n",
    "    }\n",
    "}\n",
    "py.iplot(fig, filename='hoiii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  This plot is similar to the one before, but in this case the predicted Y is on the axis. It shows that the model has problems with predicting higher Y values and it only predicts untill 6000 for the train set. For the test set, this problem is even worse and the maximum is aprroximately 4000.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame({'feature':X_Train.columns,'importance':np.round(rf.feature_importances_,3)})\n",
    "importances = importances.sort_values('importance',ascending=False).set_index('feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X.columns.values\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='#8f63f4', align='center')\n",
    "plt.yticks(range(len(indices)), features[indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">  The feature importances plot shows which column play an important role in the random random forest. From this plot we can\n",
    "conclude that, according to our rf model, income, bmi and age play an important role in determining the out of pocket \n",
    "expenditures for a person. With these feature importances, it might be good to get more insights in the important features.\n",
    "After intepreting the results of feature importance, it could be useful to remove features that are not important for predicting the model. In this case, we do not have so many features and therefore we do not apply this. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = X_Train.values[None,0]; row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a prediction for out of pocket medical expenditures based on the same characteristics as for the multivariate linear regression:\n",
    "\n",
    "- age = 82 years\n",
    "- gender = male\n",
    "- race = black/African American\n",
    "- education = 10 years\n",
    "- income = 25000$\n",
    "- children = 2\n",
    "- health = very good\n",
    "- bmi = 24\n",
    "- alcohol = 3 days a week)\n",
    "- smoking = yes\n",
    "- covered_government = yes\n",
    "- covered_employer = no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.predict([82,1,1,0,10,25000,4,1,0,0,0,24,0,0,1,0,0,0,0,1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> \n",
    "\n",
    "Text\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.predict([82,1,1,0,10,25000,4,1,0,0,0,32,0,0,1,0,0,0,0,1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Discussion'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> \n",
    "This study has several limitations. \n",
    "<br><br>\n",
    "_Low prediction scores_<br>\n",
    "Overall, the prediction score of the three models were relatively low compared to what we encountered in other papers. Our models are not able to predict out of pocket medical expenditures of elderly in the US accurately. \n",
    "<br><br>\n",
    "\n",
    "_Limited features_<br>\n",
    "    \n",
    "The models that are constructed might be biased toward features that are known to us. The set of features we used in this assignment was also influenced by the availibity of data in the HRS. So there might be relationships that our database is not aware of. For example, we include socio-dempgraphic and medical features into the models, whereas employment history might be relevant as well. <br><br>\n",
    "\n",
    "Furthermore, we only used data from 2014. An analysis over time would be interesting as well. Panel data including medical, employment, and family history could improve predictions. <br><br>\n",
    "\n",
    "\n",
    "_Limited models_<br>\n",
    "\n",
    "Given the models’ high MSE and the low R2, it appears that the causation of out of pocket medical expenditures is more complex than our models capture. There are more algorithms than we could investigate, and other machine learning approaches should be explored as tools for studying determinants of ut of pocket medical expenditures. In particular, while we consider parametric and nonparametric models, we do not consider semiparametric models which may offer flexibility beyond parametric models like LASSO without the challenges of interpretation faced by random forests. \n",
    "<br><br>\n",
    "\n",
    "_Complicated sector_<br>\n",
    "\n",
    "The health care sector, especially in the United States, is constantly evolving and there are a lot of factors at play. There have been a lot of reforms over the years and the design of the health insurance system is very complex. This of course could change the outcome of our models. <br><br>\n",
    "\n",
    "\n",
    "_External validity_<br>\n",
    "\n",
    "The prediction models that we created may not generalize to other countries because the United States has a unique institutional background, health care system, and culture. Nevertheless, we expect that our approach of constructing the models should be generalizable. It just would have to be applied to other data, and possibly other features should go into the models.<br><br>\n",
    "\n",
    "\n",
    "_Education_<br>\n",
    "The maximum numbers of years of education in our dataset is 18 years. This is typical for the group that we are analyzing, but nowadays it is not realistic anymore. <br><br>\n",
    "\n",
    "\n",
    "Of course, future research can address some of these limitations. Additional data can be collected to evaluate predictive performance, and other models could be explored. Futhermore, the analysis could be expanded to other demographic groups, countries, and insurances. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Conclusion'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> \n",
    "This assignment predicts out of pocket medical expenditures of elderly between the age of 73 and 83 years old in the United States. More specifically, the effect of age, gender, race, education, income, number of children, self-reported health, BMI, alcohol, smoking, coverage by the government, and government by a former employer are explored. We made predictions by applying a multivariate linear regression, lasso regression, and random forest model  to data from the US Health and Retirement Study.\n",
    "<br>\n",
    "<br>\n",
    "Understanding the likely success of policies will depend, in part, on being able to identify the most important features. Let us therefore translate the findings from this research into practice. The predictive models provide some guidance on the preventive factors needed to inform interventions. First, governments have to significantly increase public spending on health. Coverage by the government can substantially decrease out of pocket medical expenditures. Futhermore, increasing health is key to reduction.\n",
    "<br>\n",
    "<br>\n",
    "When assessing which personal characteristics influence out-of pocket medical expenditures most, we see different results in our models. The linear and lasso regression model indicate that self-reported health coefficients are most influential on out of pocket medical expenditures, while income had the least impact. In contrast, the random forest model **XXX**. \n",
    "<br>\n",
    "<br>\n",
    "Understanding the likely success of policies will depend, in part, on being able to identify the most important features. Let us therefore translate the findings from this research into practice. The predictive models provide some guidance on the preventive factors needed to inform interventions. First, governments have to significantly increase public spending on health. Coverage by the government can substantially decrease out of pocket medical expenditures. Second, increasing health is key to reduction. Prevention programmes are among the possibilities.\n",
    "<br><br>\n",
    "$MSE = \\displaystyle\\frac{\\sum (Y_i-Y_i)^2}{n}$\n",
    "<br><br>\n",
    "We can also look at the R-square of our models:\n",
    "<br>\n",
    "We implemented multivariate linear regression with an accuracy of 2% based on the test set.\n",
    "<br>\n",
    "We implemented a lasso regression model with an accuracy of 3.6% based on the test set.\n",
    "<br>\n",
    "We implemented Random Forest with an accuracy of nearly 0% based on the test set.\n",
    "<br>\n",
    "<br>\n",
    "We find that our three machine learning approaches do not typically perform better than simpler models for prediction.\n",
    "<br>\n",
    "<br>\n",
    "In general based on the characteristics we used to predict gives the random forest model a higher prediction for out of pocket medical expenditures compared to the multivariate linear regression model. If “being covered by the government” and “bmi” changes, they do move in the same direction which shows that they move in a comparable way. However the outcome in the random forest model shows a larger change than the multivariate model. <br><br>\n",
    "As expected, our models do really show comparable results since they both experience problems with the accuracy of the model. \n",
    "<br><br>\n",
    "To conclude, out-of pocket medical expenditures of elderly in the United States for individuals could be predicted by supervised machine learning techniques, but not with the models and/or features used in this assignment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#Back to top'>Back to top</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
